{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bf801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step0: Import_The_libraries, Prepare_your_data, Initialise_hyperparameters, Set_the_device\n",
    "#step1: Define_Neural_network\n",
    "#step2: Define_loss_Function_and_optimizer\n",
    "#step3: Training \n",
    "#step4: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d2173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/anaconda3/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/3e/4f/ad5c2a7d2783649c8ea691441a9f285accae922a1625e21603c45e3ddff4/torchvision-0.17.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchvision-0.17.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/anaconda3/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in /Users/anaconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.0 in /Users/anaconda3/lib/python3.11/site-packages (from torchvision) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/anaconda3/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in /Users/anaconda3/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/anaconda3/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/anaconda3/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/anaconda3/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/anaconda3/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/anaconda3/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.2.0->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/anaconda3/lib/python3.11/site-packages (from sympy->torch==2.2.0->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.17.0-cp311-cp311-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57efe916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 1648877/1648877 [00:01<00:00, 1583834.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 1844194.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data=torchvision.datasets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           download=True,\n",
    "                           transform=torchvision.transforms.ToTensor())\n",
    "testing_data=torchvision.datasets.MNIST(root='./data',\n",
    "                           download=True,\n",
    "                           transform=torchvision.transforms.ToTensor())\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0092abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c58f3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                        batch_size=128,\n",
    "                                        num_workers=2)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=testing_data,\n",
    "                                        batch_size=128,\n",
    "                                        num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1939180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=iter(train_loader)\n",
    "images, labels=next(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bda9b595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b47e2d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c11970d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(images[i][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65638e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 3\n",
    "learning_rate=0.001\n",
    "input_size=784\n",
    "hidden_unit=500\n",
    "output_unit=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bf7e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f0daa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4212a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_unit, output_unit):\n",
    "        super(Neural_Net, self).__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.l1=nn.Linear(input_size, hidden_unit)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.l2=nn.Linear(hidden_unit, output_unit)\n",
    "    def forward(self, x):\n",
    "        out=self.flatten(x)#Then_no_need_to_use_reshape\n",
    "        out=self.l1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.l2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18454b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_unit, output_unit):\n",
    "        super(Neural_Net, self).__init__()\n",
    "        self.relu_stack=nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(input_size, hidden_unit),\n",
    "        nn.ReLU,\n",
    "        nn.Linear(hidden_unit, output_unit))\n",
    "        def forward(self, x):\n",
    "            out=self.relu_stack(x)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4215ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Neural_Net(input_size, hidden_unit, output_unit).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16e784e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()#Cross_entropy_in_pytorch_automatically_take_softmax_of_values_and_one_hot_encoded_values\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba50254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0batch:0loss:tensor(0.1195, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:1loss:tensor(0.2520, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:2loss:tensor(0.1008, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:3loss:tensor(0.2256, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:4loss:tensor(0.1860, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:5loss:tensor(0.1791, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:6loss:tensor(0.2392, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:7loss:tensor(0.1987, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:8loss:tensor(0.4113, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:9loss:tensor(0.2358, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:10loss:tensor(0.2647, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:11loss:tensor(0.1172, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:12loss:tensor(0.1334, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:13loss:tensor(0.1025, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:14loss:tensor(0.0999, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:15loss:tensor(0.1698, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:16loss:tensor(0.1089, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:17loss:tensor(0.1237, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:18loss:tensor(0.1469, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:19loss:tensor(0.0636, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:20loss:tensor(0.1825, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:21loss:tensor(0.1362, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:22loss:tensor(0.1043, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:23loss:tensor(0.1717, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:24loss:tensor(0.0692, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:25loss:tensor(0.1388, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:26loss:tensor(0.0780, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:27loss:tensor(0.1377, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:28loss:tensor(0.1596, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:29loss:tensor(0.1690, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:30loss:tensor(0.0617, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:31loss:tensor(0.1675, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:32loss:tensor(0.1718, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:33loss:tensor(0.1813, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:34loss:tensor(0.1255, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:35loss:tensor(0.1088, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:36loss:tensor(0.1580, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:37loss:tensor(0.1252, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:38loss:tensor(0.1754, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:39loss:tensor(0.1267, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:40loss:tensor(0.1947, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:41loss:tensor(0.1540, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:42loss:tensor(0.0868, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:43loss:tensor(0.1504, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:44loss:tensor(0.1502, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:45loss:tensor(0.2052, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:46loss:tensor(0.1101, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:47loss:tensor(0.1336, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:48loss:tensor(0.1002, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:49loss:tensor(0.0791, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:50loss:tensor(0.1434, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:51loss:tensor(0.1084, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:52loss:tensor(0.1090, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:53loss:tensor(0.2757, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:54loss:tensor(0.2305, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:55loss:tensor(0.1423, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:56loss:tensor(0.2035, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:57loss:tensor(0.1765, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:58loss:tensor(0.1500, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:59loss:tensor(0.1508, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:60loss:tensor(0.1508, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:61loss:tensor(0.1910, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:62loss:tensor(0.1386, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:63loss:tensor(0.1332, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:64loss:tensor(0.2846, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:65loss:tensor(0.0866, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:66loss:tensor(0.2291, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:67loss:tensor(0.1749, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:68loss:tensor(0.3912, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:69loss:tensor(0.3306, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:70loss:tensor(0.1060, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:71loss:tensor(0.1646, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:72loss:tensor(0.1793, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:73loss:tensor(0.2016, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:74loss:tensor(0.1743, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:75loss:tensor(0.1097, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:76loss:tensor(0.1449, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:77loss:tensor(0.1211, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:78loss:tensor(0.1464, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:79loss:tensor(0.2095, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:80loss:tensor(0.1776, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:81loss:tensor(0.0667, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:82loss:tensor(0.0487, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:83loss:tensor(0.1971, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:84loss:tensor(0.1604, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:85loss:tensor(0.1177, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:86loss:tensor(0.1035, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:87loss:tensor(0.1067, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:88loss:tensor(0.0619, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:89loss:tensor(0.1058, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:90loss:tensor(0.2428, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:91loss:tensor(0.2373, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:92loss:tensor(0.1634, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:93loss:tensor(0.1387, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:94loss:tensor(0.0874, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:95loss:tensor(0.1248, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:96loss:tensor(0.2067, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:97loss:tensor(0.2260, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:98loss:tensor(0.4863, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:99loss:tensor(0.2454, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:100loss:tensor(0.0946, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:101loss:tensor(0.1921, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:102loss:tensor(0.1910, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:103loss:tensor(0.1462, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:104loss:tensor(0.1317, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:105loss:tensor(0.1226, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:106loss:tensor(0.1977, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:107loss:tensor(0.1794, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:108loss:tensor(0.1459, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:109loss:tensor(0.2531, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:110loss:tensor(0.1308, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:111loss:tensor(0.1980, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:112loss:tensor(0.1827, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:113loss:tensor(0.1485, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:114loss:tensor(0.1785, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:115loss:tensor(0.2563, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:116loss:tensor(0.1202, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:117loss:tensor(0.1164, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:118loss:tensor(0.2463, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:119loss:tensor(0.1470, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:120loss:tensor(0.0969, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:121loss:tensor(0.1232, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:122loss:tensor(0.1403, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:123loss:tensor(0.2142, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:124loss:tensor(0.2256, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:125loss:tensor(0.1792, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:126loss:tensor(0.0961, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:127loss:tensor(0.0693, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:128loss:tensor(0.1322, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:129loss:tensor(0.1501, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:130loss:tensor(0.1332, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:131loss:tensor(0.1396, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:132loss:tensor(0.1515, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:133loss:tensor(0.1650, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:134loss:tensor(0.1639, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:135loss:tensor(0.0842, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:136loss:tensor(0.1338, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:137loss:tensor(0.1510, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:138loss:tensor(0.2005, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:139loss:tensor(0.1473, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:140loss:tensor(0.1413, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:141loss:tensor(0.1077, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:142loss:tensor(0.0861, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:143loss:tensor(0.1223, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:144loss:tensor(0.1340, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:145loss:tensor(0.1034, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:146loss:tensor(0.1116, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:147loss:tensor(0.0714, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:148loss:tensor(0.0778, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:149loss:tensor(0.1662, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:150loss:tensor(0.1150, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:151loss:tensor(0.1527, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:152loss:tensor(0.1623, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:153loss:tensor(0.0784, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:154loss:tensor(0.1579, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:155loss:tensor(0.1264, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:156loss:tensor(0.1697, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:157loss:tensor(0.1574, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:158loss:tensor(0.1133, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:159loss:tensor(0.0620, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:160loss:tensor(0.1259, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:161loss:tensor(0.2547, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:162loss:tensor(0.2263, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:163loss:tensor(0.2513, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:164loss:tensor(0.1838, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:165loss:tensor(0.1269, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:166loss:tensor(0.1262, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:167loss:tensor(0.0907, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:168loss:tensor(0.2370, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:169loss:tensor(0.1119, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:170loss:tensor(0.0414, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:171loss:tensor(0.1424, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:172loss:tensor(0.0962, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:173loss:tensor(0.2016, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:174loss:tensor(0.0756, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:175loss:tensor(0.2400, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:176loss:tensor(0.2756, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:177loss:tensor(0.0942, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:178loss:tensor(0.1103, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:179loss:tensor(0.0730, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:180loss:tensor(0.1375, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:181loss:tensor(0.1054, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:182loss:tensor(0.1193, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:183loss:tensor(0.1133, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:184loss:tensor(0.1619, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:185loss:tensor(0.1805, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:186loss:tensor(0.1386, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:187loss:tensor(0.1565, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:188loss:tensor(0.0715, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:189loss:tensor(0.1849, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:190loss:tensor(0.0513, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:191loss:tensor(0.1181, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:192loss:tensor(0.1590, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:193loss:tensor(0.1974, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:194loss:tensor(0.1251, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:195loss:tensor(0.0459, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:196loss:tensor(0.1391, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:197loss:tensor(0.1231, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:198loss:tensor(0.0433, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:199loss:tensor(0.2093, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:200loss:tensor(0.1477, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:201loss:tensor(0.1770, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:202loss:tensor(0.0941, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:203loss:tensor(0.0796, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:204loss:tensor(0.0702, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:205loss:tensor(0.1270, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:206loss:tensor(0.2231, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:207loss:tensor(0.2049, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:208loss:tensor(0.3078, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:209loss:tensor(0.1599, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:210loss:tensor(0.1421, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:211loss:tensor(0.0668, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:212loss:tensor(0.3478, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:213loss:tensor(0.0559, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:214loss:tensor(0.1066, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:215loss:tensor(0.1232, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:216loss:tensor(0.1424, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:217loss:tensor(0.1758, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:218loss:tensor(0.0599, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:219loss:tensor(0.1187, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:220loss:tensor(0.1564, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:221loss:tensor(0.2086, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:222loss:tensor(0.0982, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:223loss:tensor(0.2256, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:224loss:tensor(0.1057, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:225loss:tensor(0.0688, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:226loss:tensor(0.1183, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:227loss:tensor(0.1275, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:228loss:tensor(0.1502, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:229loss:tensor(0.1572, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:230loss:tensor(0.0772, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:231loss:tensor(0.1069, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:232loss:tensor(0.1144, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:233loss:tensor(0.1869, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:234loss:tensor(0.1686, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:235loss:tensor(0.1695, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:236loss:tensor(0.0905, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:237loss:tensor(0.0744, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:238loss:tensor(0.1479, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:239loss:tensor(0.1460, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:240loss:tensor(0.1095, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:241loss:tensor(0.1678, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:242loss:tensor(0.1041, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:243loss:tensor(0.1890, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:244loss:tensor(0.2523, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:245loss:tensor(0.1222, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:246loss:tensor(0.1623, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:247loss:tensor(0.2614, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:248loss:tensor(0.1378, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:249loss:tensor(0.0960, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:250loss:tensor(0.1601, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:251loss:tensor(0.1124, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:252loss:tensor(0.1545, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:253loss:tensor(0.2434, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:254loss:tensor(0.0508, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:255loss:tensor(0.1526, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:256loss:tensor(0.1922, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:257loss:tensor(0.0695, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:258loss:tensor(0.1088, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:259loss:tensor(0.1012, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:260loss:tensor(0.1666, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:261loss:tensor(0.1329, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:262loss:tensor(0.0773, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:263loss:tensor(0.1606, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:264loss:tensor(0.0452, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:265loss:tensor(0.0760, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:266loss:tensor(0.1009, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:267loss:tensor(0.0476, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:268loss:tensor(0.1448, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:269loss:tensor(0.1370, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:270loss:tensor(0.1450, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:271loss:tensor(0.1789, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:272loss:tensor(0.1856, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:273loss:tensor(0.1092, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:274loss:tensor(0.1038, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:275loss:tensor(0.1476, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:276loss:tensor(0.0892, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:277loss:tensor(0.1632, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:278loss:tensor(0.1252, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:279loss:tensor(0.0442, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:280loss:tensor(0.1060, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:281loss:tensor(0.1604, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:282loss:tensor(0.1566, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:283loss:tensor(0.0591, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:284loss:tensor(0.1382, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:285loss:tensor(0.0852, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:286loss:tensor(0.0783, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:287loss:tensor(0.1321, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:288loss:tensor(0.0976, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:289loss:tensor(0.1791, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:290loss:tensor(0.1004, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:291loss:tensor(0.2223, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:292loss:tensor(0.3896, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:293loss:tensor(0.0994, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:294loss:tensor(0.1350, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:295loss:tensor(0.1805, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:296loss:tensor(0.0884, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:297loss:tensor(0.0654, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:298loss:tensor(0.0697, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:299loss:tensor(0.1696, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:300loss:tensor(0.1089, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:301loss:tensor(0.1245, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:302loss:tensor(0.1382, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:303loss:tensor(0.0346, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:304loss:tensor(0.1043, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:305loss:tensor(0.0748, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:306loss:tensor(0.1416, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:307loss:tensor(0.3147, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:308loss:tensor(0.1141, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:309loss:tensor(0.1123, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:310loss:tensor(0.0970, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:311loss:tensor(0.1226, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:312loss:tensor(0.1363, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:313loss:tensor(0.1053, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:314loss:tensor(0.0961, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:315loss:tensor(0.1040, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:316loss:tensor(0.1018, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:317loss:tensor(0.0962, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:318loss:tensor(0.1380, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:319loss:tensor(0.0752, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:320loss:tensor(0.1589, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:321loss:tensor(0.1176, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:322loss:tensor(0.2525, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:323loss:tensor(0.2064, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:324loss:tensor(0.1595, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:325loss:tensor(0.0714, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:326loss:tensor(0.1125, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:327loss:tensor(0.1318, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:328loss:tensor(0.1128, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:329loss:tensor(0.1424, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:330loss:tensor(0.1467, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:331loss:tensor(0.1901, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:332loss:tensor(0.1908, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:333loss:tensor(0.0881, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:334loss:tensor(0.1657, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:335loss:tensor(0.1922, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:336loss:tensor(0.1931, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:337loss:tensor(0.1155, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:338loss:tensor(0.0381, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:339loss:tensor(0.0638, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:340loss:tensor(0.0961, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:341loss:tensor(0.1096, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:342loss:tensor(0.1287, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:343loss:tensor(0.1367, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:344loss:tensor(0.1513, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:345loss:tensor(0.0947, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:346loss:tensor(0.1388, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:347loss:tensor(0.1928, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:348loss:tensor(0.0969, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:349loss:tensor(0.0658, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:350loss:tensor(0.1873, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:351loss:tensor(0.1405, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:352loss:tensor(0.0941, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:353loss:tensor(0.1030, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:354loss:tensor(0.0944, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:355loss:tensor(0.1697, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:356loss:tensor(0.1723, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:357loss:tensor(0.1394, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:358loss:tensor(0.1454, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:359loss:tensor(0.1520, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:360loss:tensor(0.1642, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:361loss:tensor(0.2756, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:362loss:tensor(0.2122, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:363loss:tensor(0.0409, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:364loss:tensor(0.0589, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:365loss:tensor(0.1564, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:366loss:tensor(0.0951, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:367loss:tensor(0.1347, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:368loss:tensor(0.1198, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:369loss:tensor(0.2141, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:370loss:tensor(0.1834, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:371loss:tensor(0.1661, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:372loss:tensor(0.1134, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:373loss:tensor(0.0940, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:374loss:tensor(0.1717, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:375loss:tensor(0.0974, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:376loss:tensor(0.0382, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:377loss:tensor(0.1292, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:378loss:tensor(0.0580, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:379loss:tensor(0.0929, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:380loss:tensor(0.0705, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:381loss:tensor(0.0534, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:382loss:tensor(0.3772, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:383loss:tensor(0.0940, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:384loss:tensor(0.1395, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:385loss:tensor(0.0628, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:386loss:tensor(0.2205, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:387loss:tensor(0.2461, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:388loss:tensor(0.0934, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:389loss:tensor(0.2810, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:390loss:tensor(0.1008, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:391loss:tensor(0.0948, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:392loss:tensor(0.1762, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:393loss:tensor(0.3034, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:394loss:tensor(0.1363, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:395loss:tensor(0.1262, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:396loss:tensor(0.1226, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:397loss:tensor(0.0988, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:398loss:tensor(0.0396, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:399loss:tensor(0.0721, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:400loss:tensor(0.1531, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:401loss:tensor(0.0604, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:402loss:tensor(0.1090, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:403loss:tensor(0.1033, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:404loss:tensor(0.0985, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:405loss:tensor(0.0637, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:406loss:tensor(0.1190, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:407loss:tensor(0.2097, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:408loss:tensor(0.1191, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:409loss:tensor(0.1053, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:410loss:tensor(0.0520, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:411loss:tensor(0.1139, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:412loss:tensor(0.1398, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:413loss:tensor(0.2853, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:414loss:tensor(0.0716, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:415loss:tensor(0.1182, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:416loss:tensor(0.0233, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:417loss:tensor(0.1099, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:418loss:tensor(0.1735, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:419loss:tensor(0.1846, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:420loss:tensor(0.1294, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:421loss:tensor(0.1477, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:422loss:tensor(0.0834, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:423loss:tensor(0.0895, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:424loss:tensor(0.0981, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:425loss:tensor(0.0844, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:426loss:tensor(0.0836, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:427loss:tensor(0.0578, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:428loss:tensor(0.1346, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:429loss:tensor(0.1054, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:430loss:tensor(0.0584, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:431loss:tensor(0.1239, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:432loss:tensor(0.1102, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:433loss:tensor(0.1236, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:434loss:tensor(0.0563, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:435loss:tensor(0.1463, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:436loss:tensor(0.1266, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:437loss:tensor(0.0868, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:438loss:tensor(0.0796, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:439loss:tensor(0.1276, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:440loss:tensor(0.0641, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:441loss:tensor(0.1289, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:442loss:tensor(0.1426, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:443loss:tensor(0.0753, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:444loss:tensor(0.0802, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:445loss:tensor(0.1125, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:446loss:tensor(0.0327, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:447loss:tensor(0.0984, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:448loss:tensor(0.0665, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:449loss:tensor(0.0731, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:450loss:tensor(0.1464, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:451loss:tensor(0.1003, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:452loss:tensor(0.0458, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:453loss:tensor(0.0948, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:454loss:tensor(0.0350, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:455loss:tensor(0.0360, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:456loss:tensor(0.0552, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:457loss:tensor(0.0210, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:458loss:tensor(0.0472, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:459loss:tensor(0.1908, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:460loss:tensor(0.0237, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:461loss:tensor(0.0103, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:462loss:tensor(0.0206, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:463loss:tensor(0.1409, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:464loss:tensor(0.0498, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:465loss:tensor(0.0151, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:466loss:tensor(0.3578, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:467loss:tensor(0.0161, grad_fn=<NllLossBackward0>)\n",
      "iteration:0batch:468loss:tensor(0.2026, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:0loss:tensor(0.0691, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:1loss:tensor(0.1556, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:2loss:tensor(0.0538, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:3loss:tensor(0.1706, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:4loss:tensor(0.1007, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:5loss:tensor(0.1231, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:6loss:tensor(0.1323, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:7loss:tensor(0.1068, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:8loss:tensor(0.2775, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:9loss:tensor(0.1810, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:10loss:tensor(0.1794, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:11loss:tensor(0.0590, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:12loss:tensor(0.0923, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:13loss:tensor(0.0554, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:14loss:tensor(0.0412, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:15loss:tensor(0.1182, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:16loss:tensor(0.0650, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:17loss:tensor(0.0573, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:18loss:tensor(0.0696, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:19loss:tensor(0.0343, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:20loss:tensor(0.1323, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:21loss:tensor(0.0922, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:22loss:tensor(0.0586, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:23loss:tensor(0.0938, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:24loss:tensor(0.0294, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:25loss:tensor(0.0744, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:26loss:tensor(0.0476, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:27loss:tensor(0.0910, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:28loss:tensor(0.0944, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:29loss:tensor(0.0896, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:30loss:tensor(0.0292, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:31loss:tensor(0.0950, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:32loss:tensor(0.0901, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:33loss:tensor(0.1156, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:34loss:tensor(0.0947, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:35loss:tensor(0.0573, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:36loss:tensor(0.1078, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:37loss:tensor(0.0582, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:38loss:tensor(0.1102, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:39loss:tensor(0.1129, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:40loss:tensor(0.1062, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:41loss:tensor(0.0989, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:42loss:tensor(0.0329, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:43loss:tensor(0.0772, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:44loss:tensor(0.0984, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:45loss:tensor(0.1345, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:46loss:tensor(0.0806, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:47loss:tensor(0.0879, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:48loss:tensor(0.0777, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:49loss:tensor(0.0366, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:50loss:tensor(0.0801, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:51loss:tensor(0.0669, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:52loss:tensor(0.0569, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:53loss:tensor(0.2105, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:54loss:tensor(0.1510, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:55loss:tensor(0.1078, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:56loss:tensor(0.1277, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:57loss:tensor(0.1030, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:58loss:tensor(0.0801, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:59loss:tensor(0.1057, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:60loss:tensor(0.0845, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:61loss:tensor(0.1072, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:62loss:tensor(0.0657, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:63loss:tensor(0.0876, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:64loss:tensor(0.2050, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:65loss:tensor(0.0548, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:66loss:tensor(0.1250, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:67loss:tensor(0.1083, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:68loss:tensor(0.2581, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:69loss:tensor(0.2124, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:70loss:tensor(0.0766, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:71loss:tensor(0.0916, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:72loss:tensor(0.1307, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:73loss:tensor(0.1150, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:74loss:tensor(0.0796, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:75loss:tensor(0.0582, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:76loss:tensor(0.0828, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:77loss:tensor(0.0721, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:78loss:tensor(0.0941, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:79loss:tensor(0.1423, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:80loss:tensor(0.1300, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:81loss:tensor(0.0372, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:82loss:tensor(0.0195, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:83loss:tensor(0.0996, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:84loss:tensor(0.0878, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:85loss:tensor(0.0848, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:86loss:tensor(0.0700, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:87loss:tensor(0.0743, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:88loss:tensor(0.0339, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:89loss:tensor(0.0607, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:90loss:tensor(0.1479, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:91loss:tensor(0.1574, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:92loss:tensor(0.1297, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:93loss:tensor(0.1080, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:94loss:tensor(0.0393, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:95loss:tensor(0.0701, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:96loss:tensor(0.1235, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:97loss:tensor(0.1210, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:98loss:tensor(0.3056, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:99loss:tensor(0.1415, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:100loss:tensor(0.0555, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:101loss:tensor(0.1005, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:102loss:tensor(0.0923, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:103loss:tensor(0.0920, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:104loss:tensor(0.0808, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:105loss:tensor(0.0739, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:106loss:tensor(0.1296, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:107loss:tensor(0.1175, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:108loss:tensor(0.0728, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:109loss:tensor(0.1463, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:110loss:tensor(0.0847, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:111loss:tensor(0.1118, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:112loss:tensor(0.1023, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:113loss:tensor(0.0867, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:114loss:tensor(0.0888, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:115loss:tensor(0.1449, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:116loss:tensor(0.0568, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:117loss:tensor(0.0584, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:118loss:tensor(0.1465, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:119loss:tensor(0.1098, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:120loss:tensor(0.0742, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:121loss:tensor(0.0736, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:122loss:tensor(0.0854, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:123loss:tensor(0.1555, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:124loss:tensor(0.1398, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:125loss:tensor(0.1182, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:126loss:tensor(0.0727, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:127loss:tensor(0.0556, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:128loss:tensor(0.0966, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:129loss:tensor(0.0919, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:130loss:tensor(0.0910, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:131loss:tensor(0.0746, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:132loss:tensor(0.0995, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:133loss:tensor(0.1100, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:134loss:tensor(0.1103, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:135loss:tensor(0.0571, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:136loss:tensor(0.0751, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:137loss:tensor(0.0972, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:138loss:tensor(0.1506, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:139loss:tensor(0.0930, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:140loss:tensor(0.0910, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:141loss:tensor(0.0752, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:142loss:tensor(0.0586, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:143loss:tensor(0.0852, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:144loss:tensor(0.0727, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:145loss:tensor(0.0629, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:146loss:tensor(0.0608, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:147loss:tensor(0.0291, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:148loss:tensor(0.0403, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:149loss:tensor(0.1176, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:150loss:tensor(0.0578, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:151loss:tensor(0.1390, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:152loss:tensor(0.1004, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:153loss:tensor(0.0481, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:154loss:tensor(0.0861, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:155loss:tensor(0.0891, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:156loss:tensor(0.1088, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:157loss:tensor(0.0840, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:158loss:tensor(0.0753, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:159loss:tensor(0.0289, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:160loss:tensor(0.0810, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:161loss:tensor(0.1889, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:162loss:tensor(0.1683, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:163loss:tensor(0.1496, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:164loss:tensor(0.1081, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:165loss:tensor(0.0826, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:166loss:tensor(0.0605, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:167loss:tensor(0.0570, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:168loss:tensor(0.1740, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:169loss:tensor(0.0622, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:170loss:tensor(0.0293, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:171loss:tensor(0.1007, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:172loss:tensor(0.0646, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:173loss:tensor(0.1509, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:174loss:tensor(0.0567, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:175loss:tensor(0.1587, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:176loss:tensor(0.1893, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:177loss:tensor(0.0623, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:178loss:tensor(0.0690, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:179loss:tensor(0.0528, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:180loss:tensor(0.0776, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:181loss:tensor(0.0754, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:182loss:tensor(0.0827, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:183loss:tensor(0.0571, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:184loss:tensor(0.1326, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:185loss:tensor(0.1030, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:186loss:tensor(0.0983, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:187loss:tensor(0.1126, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:188loss:tensor(0.0448, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:189loss:tensor(0.1241, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:190loss:tensor(0.0216, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:191loss:tensor(0.0846, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:192loss:tensor(0.0800, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:193loss:tensor(0.1151, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:194loss:tensor(0.0710, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:195loss:tensor(0.0221, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:196loss:tensor(0.1057, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:197loss:tensor(0.0938, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:198loss:tensor(0.0215, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:199loss:tensor(0.1542, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:200loss:tensor(0.0872, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:201loss:tensor(0.1107, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:202loss:tensor(0.0599, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:203loss:tensor(0.0549, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:204loss:tensor(0.0356, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:205loss:tensor(0.0801, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:206loss:tensor(0.1577, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:207loss:tensor(0.1825, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:208loss:tensor(0.2014, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:209loss:tensor(0.1037, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:210loss:tensor(0.1154, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:211loss:tensor(0.0373, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:212loss:tensor(0.2584, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:213loss:tensor(0.0294, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:214loss:tensor(0.0735, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:215loss:tensor(0.0972, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:216loss:tensor(0.0930, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:217loss:tensor(0.1264, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:218loss:tensor(0.0328, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:219loss:tensor(0.0615, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:220loss:tensor(0.0928, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:221loss:tensor(0.1409, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:222loss:tensor(0.0671, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:223loss:tensor(0.1401, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:224loss:tensor(0.0753, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:225loss:tensor(0.0458, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:226loss:tensor(0.0831, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:227loss:tensor(0.0710, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:228loss:tensor(0.1039, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:229loss:tensor(0.0946, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:230loss:tensor(0.0478, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:231loss:tensor(0.0690, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:232loss:tensor(0.0692, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:233loss:tensor(0.1149, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:234loss:tensor(0.1110, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:235loss:tensor(0.0975, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:236loss:tensor(0.0500, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:237loss:tensor(0.0411, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:238loss:tensor(0.0895, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:239loss:tensor(0.0925, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:240loss:tensor(0.0679, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:241loss:tensor(0.1093, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:242loss:tensor(0.0621, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:243loss:tensor(0.1321, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:244loss:tensor(0.1482, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:245loss:tensor(0.0743, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:246loss:tensor(0.1180, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:247loss:tensor(0.1796, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:248loss:tensor(0.0926, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:249loss:tensor(0.0567, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:250loss:tensor(0.1053, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:251loss:tensor(0.0709, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:252loss:tensor(0.0908, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:253loss:tensor(0.1648, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:254loss:tensor(0.0401, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:255loss:tensor(0.1044, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:256loss:tensor(0.1354, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:257loss:tensor(0.0353, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:258loss:tensor(0.0629, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:259loss:tensor(0.0601, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:260loss:tensor(0.1264, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:261loss:tensor(0.0900, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:262loss:tensor(0.0450, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:263loss:tensor(0.1109, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:264loss:tensor(0.0254, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:265loss:tensor(0.0394, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:266loss:tensor(0.0766, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:267loss:tensor(0.0291, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:268loss:tensor(0.1140, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:269loss:tensor(0.0798, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:270loss:tensor(0.1062, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:271loss:tensor(0.1278, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:272loss:tensor(0.1303, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:273loss:tensor(0.0844, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:274loss:tensor(0.0604, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:275loss:tensor(0.1046, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:276loss:tensor(0.0673, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:277loss:tensor(0.1342, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:278loss:tensor(0.1029, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:279loss:tensor(0.0250, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:280loss:tensor(0.0707, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:281loss:tensor(0.0955, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:282loss:tensor(0.1369, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:283loss:tensor(0.0337, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:284loss:tensor(0.1045, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:285loss:tensor(0.0429, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:286loss:tensor(0.0572, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:287loss:tensor(0.0912, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:288loss:tensor(0.0637, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:289loss:tensor(0.1393, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:290loss:tensor(0.0590, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:291loss:tensor(0.1436, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:292loss:tensor(0.2885, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:293loss:tensor(0.0464, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:294loss:tensor(0.0874, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:295loss:tensor(0.1525, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:296loss:tensor(0.0470, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:297loss:tensor(0.0391, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:298loss:tensor(0.0439, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:299loss:tensor(0.1130, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:300loss:tensor(0.0679, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:301loss:tensor(0.1010, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:302loss:tensor(0.0905, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:303loss:tensor(0.0205, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:304loss:tensor(0.0796, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:305loss:tensor(0.0363, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:306loss:tensor(0.1045, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:307loss:tensor(0.2071, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:308loss:tensor(0.0738, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:309loss:tensor(0.0783, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:310loss:tensor(0.0651, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:311loss:tensor(0.0947, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:312loss:tensor(0.0780, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:313loss:tensor(0.0658, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:314loss:tensor(0.0648, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:315loss:tensor(0.0648, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:316loss:tensor(0.0473, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:317loss:tensor(0.0599, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:318loss:tensor(0.1047, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:319loss:tensor(0.0521, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:320loss:tensor(0.1151, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:321loss:tensor(0.0847, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:322loss:tensor(0.1746, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:323loss:tensor(0.1303, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:324loss:tensor(0.1192, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:325loss:tensor(0.0444, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:326loss:tensor(0.0778, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:327loss:tensor(0.1051, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:328loss:tensor(0.0538, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:329loss:tensor(0.0685, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:330loss:tensor(0.0981, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:331loss:tensor(0.1177, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:332loss:tensor(0.1543, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:333loss:tensor(0.0515, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:334loss:tensor(0.1105, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:335loss:tensor(0.1271, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:336loss:tensor(0.1336, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:337loss:tensor(0.0550, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:338loss:tensor(0.0236, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:339loss:tensor(0.0524, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:340loss:tensor(0.0763, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:341loss:tensor(0.0907, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:342loss:tensor(0.0956, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:343loss:tensor(0.0924, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:344loss:tensor(0.1064, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:345loss:tensor(0.0531, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:346loss:tensor(0.0977, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:347loss:tensor(0.1579, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:348loss:tensor(0.0763, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:349loss:tensor(0.0343, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:350loss:tensor(0.1198, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:351loss:tensor(0.0940, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:352loss:tensor(0.0687, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:353loss:tensor(0.0845, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:354loss:tensor(0.0673, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:355loss:tensor(0.1109, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:356loss:tensor(0.1275, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:357loss:tensor(0.0945, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:358loss:tensor(0.0980, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:359loss:tensor(0.1122, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:360loss:tensor(0.1117, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:361loss:tensor(0.2091, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:362loss:tensor(0.1402, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:363loss:tensor(0.0264, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:364loss:tensor(0.0401, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:365loss:tensor(0.1181, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:366loss:tensor(0.0643, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:367loss:tensor(0.1148, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:368loss:tensor(0.0793, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:369loss:tensor(0.1676, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:370loss:tensor(0.1391, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:371loss:tensor(0.1255, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:372loss:tensor(0.0791, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:373loss:tensor(0.0593, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:374loss:tensor(0.1270, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:375loss:tensor(0.0660, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:376loss:tensor(0.0219, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:377loss:tensor(0.0700, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:378loss:tensor(0.0337, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:379loss:tensor(0.0703, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:380loss:tensor(0.0420, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:381loss:tensor(0.0485, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:382loss:tensor(0.2632, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:383loss:tensor(0.0628, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:384loss:tensor(0.0888, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:385loss:tensor(0.0408, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:386loss:tensor(0.1313, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:387loss:tensor(0.1722, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:388loss:tensor(0.0628, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:389loss:tensor(0.1973, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:390loss:tensor(0.0618, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:391loss:tensor(0.0674, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:392loss:tensor(0.1199, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:393loss:tensor(0.1916, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:394loss:tensor(0.1086, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:395loss:tensor(0.0532, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:396loss:tensor(0.0832, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:397loss:tensor(0.0873, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:398loss:tensor(0.0229, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:399loss:tensor(0.0506, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:400loss:tensor(0.1239, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:401loss:tensor(0.0443, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:402loss:tensor(0.0731, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:403loss:tensor(0.0712, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:404loss:tensor(0.0683, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:405loss:tensor(0.0344, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:406loss:tensor(0.0801, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:407loss:tensor(0.1273, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:408loss:tensor(0.0856, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:409loss:tensor(0.0682, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:410loss:tensor(0.0388, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:411loss:tensor(0.0703, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:412loss:tensor(0.0898, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:413loss:tensor(0.1995, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:414loss:tensor(0.0499, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:415loss:tensor(0.0798, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:416loss:tensor(0.0146, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:417loss:tensor(0.0793, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:418loss:tensor(0.1144, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:419loss:tensor(0.1243, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:420loss:tensor(0.0661, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:421loss:tensor(0.0821, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:422loss:tensor(0.0590, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:423loss:tensor(0.0680, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:424loss:tensor(0.0509, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:425loss:tensor(0.0706, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:426loss:tensor(0.0607, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:427loss:tensor(0.0330, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:428loss:tensor(0.0985, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:429loss:tensor(0.0670, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:430loss:tensor(0.0318, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:431loss:tensor(0.0809, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:432loss:tensor(0.0893, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:433loss:tensor(0.0960, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:434loss:tensor(0.0456, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:435loss:tensor(0.1036, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:436loss:tensor(0.0920, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:437loss:tensor(0.0732, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:438loss:tensor(0.0569, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:439loss:tensor(0.0874, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:440loss:tensor(0.0438, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:441loss:tensor(0.0828, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:442loss:tensor(0.0886, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:443loss:tensor(0.0591, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:444loss:tensor(0.0560, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:445loss:tensor(0.0863, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:446loss:tensor(0.0226, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:447loss:tensor(0.0885, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:448loss:tensor(0.0497, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:449loss:tensor(0.0475, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:450loss:tensor(0.0910, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:451loss:tensor(0.0736, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:452loss:tensor(0.0333, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:453loss:tensor(0.0724, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:454loss:tensor(0.0197, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:455loss:tensor(0.0205, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:456loss:tensor(0.0376, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:457loss:tensor(0.0129, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:458loss:tensor(0.0320, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:459loss:tensor(0.1353, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:460loss:tensor(0.0106, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:461loss:tensor(0.0060, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:462loss:tensor(0.0087, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:463loss:tensor(0.1054, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:464loss:tensor(0.0364, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:465loss:tensor(0.0112, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:466loss:tensor(0.2840, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:467loss:tensor(0.0092, grad_fn=<NllLossBackward0>)\n",
      "iteration:1batch:468loss:tensor(0.1716, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:0loss:tensor(0.0458, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:1loss:tensor(0.1176, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:2loss:tensor(0.0345, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:3loss:tensor(0.1490, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:4loss:tensor(0.0703, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:5loss:tensor(0.0960, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:6loss:tensor(0.0820, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:7loss:tensor(0.0719, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:8loss:tensor(0.1924, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:9loss:tensor(0.1323, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:10loss:tensor(0.1390, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:11loss:tensor(0.0338, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:12loss:tensor(0.0751, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:13loss:tensor(0.0390, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:14loss:tensor(0.0246, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:15loss:tensor(0.0913, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:16loss:tensor(0.0354, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:17loss:tensor(0.0356, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:18loss:tensor(0.0425, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:19loss:tensor(0.0261, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:20loss:tensor(0.1044, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:21loss:tensor(0.0728, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:22loss:tensor(0.0316, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:23loss:tensor(0.0636, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:24loss:tensor(0.0168, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:25loss:tensor(0.0468, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:26loss:tensor(0.0350, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:27loss:tensor(0.0676, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:28loss:tensor(0.0647, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:29loss:tensor(0.0589, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:30loss:tensor(0.0180, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:31loss:tensor(0.0631, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:32loss:tensor(0.0560, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:33loss:tensor(0.0642, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:34loss:tensor(0.0798, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:35loss:tensor(0.0318, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:36loss:tensor(0.0865, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:37loss:tensor(0.0275, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:38loss:tensor(0.0731, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:39loss:tensor(0.0950, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:40loss:tensor(0.0854, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:41loss:tensor(0.0748, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:42loss:tensor(0.0170, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:43loss:tensor(0.0483, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:44loss:tensor(0.0827, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:45loss:tensor(0.1002, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:46loss:tensor(0.0570, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:47loss:tensor(0.0540, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:48loss:tensor(0.0576, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:49loss:tensor(0.0206, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:50loss:tensor(0.0491, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:51loss:tensor(0.0450, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:52loss:tensor(0.0388, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:53loss:tensor(0.1776, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:54loss:tensor(0.1142, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:55loss:tensor(0.0936, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:56loss:tensor(0.0913, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:57loss:tensor(0.0779, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:58loss:tensor(0.0487, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:59loss:tensor(0.0899, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:60loss:tensor(0.0605, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:61loss:tensor(0.0743, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:62loss:tensor(0.0481, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:63loss:tensor(0.0621, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:64loss:tensor(0.1652, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:65loss:tensor(0.0374, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:66loss:tensor(0.0792, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:67loss:tensor(0.0728, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:68loss:tensor(0.1883, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:69loss:tensor(0.1545, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:70loss:tensor(0.0619, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:71loss:tensor(0.0583, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:72loss:tensor(0.1057, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:73loss:tensor(0.0773, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:74loss:tensor(0.0462, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:75loss:tensor(0.0360, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:76loss:tensor(0.0548, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:77loss:tensor(0.0470, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:78loss:tensor(0.0645, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:79loss:tensor(0.0991, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:80loss:tensor(0.0977, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:81loss:tensor(0.0286, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:82loss:tensor(0.0108, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:83loss:tensor(0.0506, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:84loss:tensor(0.0557, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:85loss:tensor(0.0686, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:86loss:tensor(0.0387, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:87loss:tensor(0.0618, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:88loss:tensor(0.0209, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:89loss:tensor(0.0347, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:90loss:tensor(0.0967, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:91loss:tensor(0.1082, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:92loss:tensor(0.0944, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:93loss:tensor(0.0937, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:94loss:tensor(0.0197, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:95loss:tensor(0.0460, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:96loss:tensor(0.0744, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:97loss:tensor(0.0649, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:98loss:tensor(0.1878, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:99loss:tensor(0.0939, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:100loss:tensor(0.0379, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:101loss:tensor(0.0567, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:102loss:tensor(0.0486, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:103loss:tensor(0.0575, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:104loss:tensor(0.0580, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:105loss:tensor(0.0508, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:106loss:tensor(0.0909, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:107loss:tensor(0.0732, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:108loss:tensor(0.0469, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:109loss:tensor(0.0952, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:110loss:tensor(0.0623, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:111loss:tensor(0.0676, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:112loss:tensor(0.0681, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:113loss:tensor(0.0589, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:114loss:tensor(0.0557, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:115loss:tensor(0.0932, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:116loss:tensor(0.0303, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:117loss:tensor(0.0359, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:118loss:tensor(0.0938, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:119loss:tensor(0.0848, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:120loss:tensor(0.0585, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:121loss:tensor(0.0525, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:122loss:tensor(0.0500, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:123loss:tensor(0.1311, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:124loss:tensor(0.0880, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:125loss:tensor(0.0737, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:126loss:tensor(0.0565, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:127loss:tensor(0.0553, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:128loss:tensor(0.0705, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:129loss:tensor(0.0580, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:130loss:tensor(0.0829, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:131loss:tensor(0.0451, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:132loss:tensor(0.0662, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:133loss:tensor(0.0811, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:134loss:tensor(0.0900, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:135loss:tensor(0.0445, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:136loss:tensor(0.0496, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:137loss:tensor(0.0642, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:138loss:tensor(0.1193, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:139loss:tensor(0.0713, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:140loss:tensor(0.0602, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:141loss:tensor(0.0601, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:142loss:tensor(0.0477, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:143loss:tensor(0.0632, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:144loss:tensor(0.0476, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:145loss:tensor(0.0497, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:146loss:tensor(0.0421, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:147loss:tensor(0.0150, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:148loss:tensor(0.0252, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:149loss:tensor(0.0909, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:150loss:tensor(0.0321, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:151loss:tensor(0.1187, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:152loss:tensor(0.0787, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:153loss:tensor(0.0333, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:154loss:tensor(0.0511, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:155loss:tensor(0.0604, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:156loss:tensor(0.0790, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:157loss:tensor(0.0547, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:158loss:tensor(0.0633, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:159loss:tensor(0.0156, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:160loss:tensor(0.0539, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:161loss:tensor(0.1483, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:162loss:tensor(0.1301, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:163loss:tensor(0.0900, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:164loss:tensor(0.0641, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:165loss:tensor(0.0591, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:166loss:tensor(0.0365, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:167loss:tensor(0.0360, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:168loss:tensor(0.1375, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:169loss:tensor(0.0356, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:170loss:tensor(0.0155, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:171loss:tensor(0.0815, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:172loss:tensor(0.0436, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:173loss:tensor(0.1211, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:174loss:tensor(0.0496, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:175loss:tensor(0.1031, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:176loss:tensor(0.1289, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:177loss:tensor(0.0410, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:178loss:tensor(0.0396, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:179loss:tensor(0.0347, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:180loss:tensor(0.0461, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:181loss:tensor(0.0564, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:182loss:tensor(0.0652, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:183loss:tensor(0.0337, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:184loss:tensor(0.1114, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:185loss:tensor(0.0646, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:186loss:tensor(0.0798, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:187loss:tensor(0.0924, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:188loss:tensor(0.0319, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:189loss:tensor(0.0839, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:190loss:tensor(0.0128, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:191loss:tensor(0.0669, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:192loss:tensor(0.0485, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:193loss:tensor(0.0686, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:194loss:tensor(0.0485, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:195loss:tensor(0.0139, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:196loss:tensor(0.0814, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:197loss:tensor(0.0771, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:198loss:tensor(0.0119, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:199loss:tensor(0.1245, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:200loss:tensor(0.0558, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:201loss:tensor(0.0718, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:202loss:tensor(0.0429, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:203loss:tensor(0.0387, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:204loss:tensor(0.0241, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:205loss:tensor(0.0607, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:206loss:tensor(0.1167, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:207loss:tensor(0.1616, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:208loss:tensor(0.1527, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:209loss:tensor(0.0714, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:210loss:tensor(0.1003, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:211loss:tensor(0.0232, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:212loss:tensor(0.1968, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:213loss:tensor(0.0194, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:214loss:tensor(0.0595, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:215loss:tensor(0.0783, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:216loss:tensor(0.0654, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:217loss:tensor(0.1058, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:218loss:tensor(0.0251, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:219loss:tensor(0.0426, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:220loss:tensor(0.0695, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:221loss:tensor(0.1037, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:222loss:tensor(0.0414, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:223loss:tensor(0.0979, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:224loss:tensor(0.0642, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:225loss:tensor(0.0388, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:226loss:tensor(0.0682, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:227loss:tensor(0.0527, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:228loss:tensor(0.0827, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:229loss:tensor(0.0727, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:230loss:tensor(0.0348, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:231loss:tensor(0.0524, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:232loss:tensor(0.0485, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:233loss:tensor(0.0720, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:234loss:tensor(0.0762, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:235loss:tensor(0.0647, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:236loss:tensor(0.0293, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:237loss:tensor(0.0274, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:238loss:tensor(0.0668, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:239loss:tensor(0.0667, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:240loss:tensor(0.0459, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:241loss:tensor(0.0712, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:242loss:tensor(0.0399, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:243loss:tensor(0.1123, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:244loss:tensor(0.0836, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:245loss:tensor(0.0489, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:246loss:tensor(0.0820, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:247loss:tensor(0.1055, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:248loss:tensor(0.0739, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:249loss:tensor(0.0447, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:250loss:tensor(0.0842, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:251loss:tensor(0.0521, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:252loss:tensor(0.0662, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:253loss:tensor(0.1066, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:254loss:tensor(0.0343, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:255loss:tensor(0.0735, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:256loss:tensor(0.0724, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:257loss:tensor(0.0223, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:258loss:tensor(0.0366, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:259loss:tensor(0.0376, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:260loss:tensor(0.0968, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:261loss:tensor(0.0719, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:262loss:tensor(0.0306, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:263loss:tensor(0.0696, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:264loss:tensor(0.0154, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:265loss:tensor(0.0233, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:266loss:tensor(0.0644, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:267loss:tensor(0.0176, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:268loss:tensor(0.0966, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:269loss:tensor(0.0531, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:270loss:tensor(0.0762, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:271loss:tensor(0.1080, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:272loss:tensor(0.0998, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:273loss:tensor(0.0692, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:274loss:tensor(0.0451, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:275loss:tensor(0.0804, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:276loss:tensor(0.0549, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:277loss:tensor(0.1093, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:278loss:tensor(0.0891, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:279loss:tensor(0.0166, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:280loss:tensor(0.0522, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:281loss:tensor(0.0727, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:282loss:tensor(0.1120, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:283loss:tensor(0.0237, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:284loss:tensor(0.0875, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:285loss:tensor(0.0197, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:286loss:tensor(0.0424, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:287loss:tensor(0.0702, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:288loss:tensor(0.0481, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:289loss:tensor(0.1128, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:290loss:tensor(0.0423, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:291loss:tensor(0.0922, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:292loss:tensor(0.1977, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:293loss:tensor(0.0282, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:294loss:tensor(0.0663, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:295loss:tensor(0.1349, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:296loss:tensor(0.0257, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:297loss:tensor(0.0231, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:298loss:tensor(0.0332, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:299loss:tensor(0.0800, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:300loss:tensor(0.0460, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:301loss:tensor(0.0899, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:302loss:tensor(0.0595, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:303loss:tensor(0.0125, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:304loss:tensor(0.0676, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:305loss:tensor(0.0213, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:306loss:tensor(0.0798, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:307loss:tensor(0.1573, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:308loss:tensor(0.0465, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:309loss:tensor(0.0551, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:310loss:tensor(0.0524, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:311loss:tensor(0.0666, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:312loss:tensor(0.0506, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:313loss:tensor(0.0515, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:314loss:tensor(0.0464, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:315loss:tensor(0.0411, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:316loss:tensor(0.0215, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:317loss:tensor(0.0379, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:318loss:tensor(0.0798, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:319loss:tensor(0.0396, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:320loss:tensor(0.0884, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:321loss:tensor(0.0666, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:322loss:tensor(0.1238, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:323loss:tensor(0.0744, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:324loss:tensor(0.0956, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:325loss:tensor(0.0313, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:326loss:tensor(0.0535, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:327loss:tensor(0.0780, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:328loss:tensor(0.0280, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:329loss:tensor(0.0377, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:330loss:tensor(0.0730, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:331loss:tensor(0.0688, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:332loss:tensor(0.1206, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:333loss:tensor(0.0324, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:334loss:tensor(0.0796, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:335loss:tensor(0.0926, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:336loss:tensor(0.1075, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:337loss:tensor(0.0350, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:338loss:tensor(0.0191, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:339loss:tensor(0.0446, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:340loss:tensor(0.0548, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:341loss:tensor(0.0780, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:342loss:tensor(0.0793, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:343loss:tensor(0.0639, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:344loss:tensor(0.0761, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:345loss:tensor(0.0293, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:346loss:tensor(0.0744, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:347loss:tensor(0.1265, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:348loss:tensor(0.0585, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:349loss:tensor(0.0223, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:350loss:tensor(0.0721, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:351loss:tensor(0.0530, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:352loss:tensor(0.0554, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:353loss:tensor(0.0594, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:354loss:tensor(0.0509, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:355loss:tensor(0.0808, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:356loss:tensor(0.0852, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:357loss:tensor(0.0551, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:358loss:tensor(0.0731, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:359loss:tensor(0.0792, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:360loss:tensor(0.0763, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:361loss:tensor(0.1479, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:362loss:tensor(0.0835, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:363loss:tensor(0.0187, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:364loss:tensor(0.0265, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:365loss:tensor(0.0962, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:366loss:tensor(0.0506, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:367loss:tensor(0.0918, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:368loss:tensor(0.0485, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:369loss:tensor(0.1385, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:370loss:tensor(0.1023, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:371loss:tensor(0.1048, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:372loss:tensor(0.0548, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:373loss:tensor(0.0384, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:374loss:tensor(0.0937, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:375loss:tensor(0.0488, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:376loss:tensor(0.0164, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:377loss:tensor(0.0474, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:378loss:tensor(0.0298, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:379loss:tensor(0.0520, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:380loss:tensor(0.0260, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:381loss:tensor(0.0455, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:382loss:tensor(0.1911, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:383loss:tensor(0.0454, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:384loss:tensor(0.0599, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:385loss:tensor(0.0262, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:386loss:tensor(0.0904, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:387loss:tensor(0.1196, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:388loss:tensor(0.0431, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:389loss:tensor(0.1420, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:390loss:tensor(0.0399, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:391loss:tensor(0.0471, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:392loss:tensor(0.0868, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:393loss:tensor(0.1309, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:394loss:tensor(0.0886, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:395loss:tensor(0.0298, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:396loss:tensor(0.0521, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:397loss:tensor(0.0741, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:398loss:tensor(0.0204, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:399loss:tensor(0.0417, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:400loss:tensor(0.1071, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:401loss:tensor(0.0369, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:402loss:tensor(0.0478, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:403loss:tensor(0.0474, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:404loss:tensor(0.0458, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:405loss:tensor(0.0217, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:406loss:tensor(0.0557, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:407loss:tensor(0.0794, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:408loss:tensor(0.0648, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:409loss:tensor(0.0511, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:410loss:tensor(0.0284, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:411loss:tensor(0.0457, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:412loss:tensor(0.0664, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:413loss:tensor(0.1357, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:414loss:tensor(0.0392, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:415loss:tensor(0.0576, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:416loss:tensor(0.0099, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:417loss:tensor(0.0553, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:418loss:tensor(0.0688, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:419loss:tensor(0.0877, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:420loss:tensor(0.0321, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:421loss:tensor(0.0466, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:422loss:tensor(0.0522, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:423loss:tensor(0.0563, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:424loss:tensor(0.0287, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:425loss:tensor(0.0614, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:426loss:tensor(0.0470, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:427loss:tensor(0.0189, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:428loss:tensor(0.0767, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:429loss:tensor(0.0473, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:430loss:tensor(0.0185, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:431loss:tensor(0.0539, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:432loss:tensor(0.0785, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:433loss:tensor(0.0692, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:434loss:tensor(0.0320, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:435loss:tensor(0.0771, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:436loss:tensor(0.0644, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:437loss:tensor(0.0510, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:438loss:tensor(0.0456, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:439loss:tensor(0.0609, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:440loss:tensor(0.0328, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:441loss:tensor(0.0610, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:442loss:tensor(0.0566, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:443loss:tensor(0.0495, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:444loss:tensor(0.0386, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:445loss:tensor(0.0657, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:446loss:tensor(0.0159, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:447loss:tensor(0.0731, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:448loss:tensor(0.0392, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:449loss:tensor(0.0423, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:450loss:tensor(0.0531, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:451loss:tensor(0.0535, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:452loss:tensor(0.0254, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:453loss:tensor(0.0567, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:454loss:tensor(0.0128, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:455loss:tensor(0.0122, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:456loss:tensor(0.0261, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:457loss:tensor(0.0097, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:458loss:tensor(0.0225, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:459loss:tensor(0.0818, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:460loss:tensor(0.0047, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:461loss:tensor(0.0045, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:462loss:tensor(0.0053, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:463loss:tensor(0.0760, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:464loss:tensor(0.0303, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:465loss:tensor(0.0106, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:466loss:tensor(0.2077, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:467loss:tensor(0.0068, grad_fn=<NllLossBackward0>)\n",
      "iteration:2batch:468loss:tensor(0.1456, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images=images.reshape(-1,784).to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs, labels)\n",
    "        #backward_propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()#otherwise_they_will_accumulated\n",
    "        print('iteration:' +str(epochs)+'batch:'+str(i)+'loss:'+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c92f3216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.385\n"
     ]
    }
   ],
   "source": [
    "n_correct=0\n",
    "n_samples=0\n",
    "for images, labels in test_loader:\n",
    "    images=images.reshape(-1,784).to(device)\n",
    "    labels=labels.to(device)\n",
    "    outputs=model(images)\n",
    "    _, predicted=torch.max(outputs, 1)\n",
    "    n_correct+=(predicted==labels).sum().item()\n",
    "    n_samples+=labels.size(0)\n",
    "Accuracy=(n_correct*100)/n_samples\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fc1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
